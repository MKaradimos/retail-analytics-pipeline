# ğŸ¢ Retail Analytics Data Pipeline

A production-ready, cloud-ready data engineering project that implements a complete ETL pipeline for retail business analytics. This project demonstrates modern data engineering practices including data ingestion, validation, transformation, and warehousing.

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-16-blue.svg)](https://www.postgresql.org/)
[![Docker](https://img.shields.io/badge/Docker-Enabled-blue.svg)](https://www.docker.com/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## ğŸ“‹ Table of Contents

- [Business Problem](#-business-problem)
- [Solution Architecture](#-solution-architecture)
- [Technical Stack](#-technical-stack)
- [Data Model](#-data-model)
- [Project Structure](#-project-structure)
- [Getting Started](#-getting-started)
- [Usage](#-usage)
- [Data Quality & Validation](#-data-quality--validation)
- [Analytics Capabilities](#-analytics-capabilities)
- [Future Enhancements](#-future-enhancements)

---

## ğŸ¯ Business Problem

A retail company needs to:
- **Consolidate** data from multiple sources (APIs, CSV files)
- **Validate and clean** incoming data to ensure quality
- **Transform** raw data into business-ready formats
- **Enable analytics** for sales performance, customer behavior, and inventory management

### Business Requirements
- Real-time product data synchronization from external APIs
- Historical sales transaction processing
- Customer analytics and segmentation
- Sales trend analysis by time, location, and category
- Data quality monitoring and validation

---

## ğŸ—ï¸ Solution Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   External API  â”‚         â”‚   CSV Files     â”‚
â”‚  (FakeStore)    â”‚         â”‚  (Sales Data)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                           â”‚
         â”‚     DATA INGESTION        â”‚
         â”‚                           â”‚
         â–¼                           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    Python Ingestion Service        â”‚
    â”‚  - API Client (requests)           â”‚
    â”‚  - CSV Parser (pandas)             â”‚
    â”‚  - Retry Logic & Error Handling    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â”‚  VALIDATION & TRANSFORMATION
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    Data Validation Layer           â”‚
    â”‚  - Pydantic Models                 â”‚
    â”‚  - Business Rules Validation       â”‚
    â”‚  - Data Quality Checks             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â”‚  WAREHOUSING
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    PostgreSQL Data Warehouse       â”‚
    â”‚  - Star Schema Design              â”‚
    â”‚  - Dimension Tables (SCD Type 1)   â”‚
    â”‚  - Fact Table (Transactions)       â”‚
    â”‚  - Pre-built Analytics Views       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **Extract**: Fetch products from REST API and load sales from CSV
2. **Validate**: Apply business rules and data quality checks
3. **Transform**: Convert to warehouse-ready format
4. **Load**: Insert into dimensional model
5. **Verify**: Run data quality checks

---

## ğŸ› ï¸ Technical Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Language** | Python 3.11 | ETL logic and orchestration |
| **Data Validation** | Pydantic | Schema validation and data models |
| **Database** | PostgreSQL 16 | Data warehouse |
| **API Client** | Requests | External API integration |
| **Data Processing** | Pandas | CSV processing and transformations |
| **Containerization** | Docker & Docker Compose | Environment consistency |
| **Database Admin** | pgAdmin 4 | Database management UI |
| **Logging** | Python logging | Pipeline monitoring |

---

## ğŸ“Š Data Model

### Star Schema Design

#### Dimension Tables

**dim_product** - Product Master Data
- `product_id` (PK)
- `product_name`
- `category`
- `description`
- `unit_price`
- `image_url`

**dim_customer** - Customer Master Data
- `customer_id` (PK)
- `customer_name`
- `email`
- `city`
- `country`
- `first_transaction_date`

**dim_date** - Time Dimension
- `date_key` (PK)
- `year`, `quarter`, `month`
- `week`, `day_of_week`
- `is_weekend`

#### Fact Table

**fact_sales** - Transaction-level Sales Data
- `transaction_id` (PK)
- `product_id` (FK)
- `customer_id` (FK)
- `date_key` (FK)
- `transaction_timestamp`
- `quantity`
- `unit_price`
- `total_amount`
- `store_location`
- `payment_method`

---

## ğŸ“ Project Structure

```
retail-analytics-pipeline/
â”œâ”€â”€ ingestion/                  # Data ingestion modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ api_ingestion.py       # API data fetching
â”‚   â””â”€â”€ csv_ingestion.py       # CSV data loading
â”œâ”€â”€ transformations/            # Data transformation logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ transform.py           # Transformation rules
â”œâ”€â”€ sql/                        # Database scripts
â”‚   â”œâ”€â”€ schema.sql             # DDL for warehouse
â”‚   â””â”€â”€ analytics_queries.sql  # Pre-built analytics
â”œâ”€â”€ data/                       # Data storage
â”‚   â”œâ”€â”€ sales_transactions.csv # Sample sales data
â”‚   â””â”€â”€ generate_sample_data.py
â”œâ”€â”€ logs/                       # Pipeline execution logs
â”œâ”€â”€ config.py                   # Configuration management
â”œâ”€â”€ database.py                 # Database utilities
â”œâ”€â”€ models.py                   # Pydantic validation models
â”œâ”€â”€ pipeline.py                 # Main ETL orchestrator
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ Dockerfile                  # Container definition
â”œâ”€â”€ docker-compose.yml          # Multi-container setup
â”œâ”€â”€ Makefile                    # Common commands
â”œâ”€â”€ .env.example               # Configuration template
â””â”€â”€ README.md                  # This file
```

---

## ğŸš€ Getting Started

### Prerequisites

- **Docker** (version 20.10+)
- **Docker Compose** (version 2.0+)
- **Python 3.11+** (for local development)
- **Git**

### Quick Start

```bash
# 1. Clone the repository
git clone <repository-url>
cd retail-analytics-pipeline

# 2. One-command setup and start
make quickstart

# 3. Run the pipeline
make run
```

That's it! The pipeline will:
- âœ… Generate sample data
- âœ… Initialize the database
- âœ… Fetch products from API
- âœ… Load transactions
- âœ… Validate data quality

### Manual Setup (Alternative)

```bash
# 1. Create environment configuration
cp .env.example .env

# 2. Generate sample data
python data/generate_sample_data.py

# 3. Build and start services
docker-compose up -d

# 4. Run pipeline
docker-compose run --rm pipeline python pipeline.py
```

---

## ğŸ’» Usage

### Common Operations

```bash
# View all available commands
make help

# Start all services
make up

# Run the ETL pipeline
make run

# View pipeline logs
make logs

# Access database shell
make psql

# Stop all services
make down

# Clean up everything
make clean
```

### Accessing Services

| Service | URL | Credentials |
|---------|-----|-------------|
| PostgreSQL | `localhost:5432` | User: `analytics_user`<br>Password: `analytics_pass_2024`<br>Database: `retail_analytics` |
| pgAdmin | `http://localhost:5050` | Email: `admin@analytics.com`<br>Password: `admin` |

### Running Analytics Queries

```bash
# Connect to database
make psql

# Run sample queries
\i /docker-entrypoint-initdb.d/analytics_queries.sql

# Example: Top selling products
SELECT * FROM vw_product_sales_summary ORDER BY total_revenue DESC LIMIT 10;

# Example: Daily sales trend
SELECT * FROM vw_daily_sales_summary ORDER BY date_key DESC LIMIT 30;
```

---

## âœ… Data Quality & Validation

### Validation Rules

**Product Data**
- âœ“ Price must be positive
- âœ“ Title cannot be empty
- âœ“ Category is required
- âœ“ Valid URL format for images

**Transaction Data**
- âœ“ Quantity must be positive
- âœ“ Amounts cannot be negative
- âœ“ Valid payment method (cash, credit_card, debit_card, online)
- âœ“ Date consistency checks
- âœ“ Foreign key integrity

### Automated Quality Checks

The pipeline runs these checks automatically:
1. **Orphan Record Detection** - Verify referential integrity
2. **Negative Amount Validation** - Ensure no negative values
3. **Date Consistency** - Verify date_key matches timestamp
4. **Duplicate Detection** - Identify duplicate transactions

### Logging

All pipeline activities are logged:
- **Location**: `logs/pipeline_YYYYMMDD_HHMMSS.log`
- **Format**: Timestamp, Logger, Level, Message
- **Levels**: INFO (normal operations), WARNING (validation issues), ERROR (failures)

---

## ğŸ“ˆ Analytics Capabilities

### Pre-built Analytics Views

**vw_product_sales_summary**
- Product performance metrics
- Total revenue and quantity sold
- Transaction counts per product

**vw_daily_sales_summary**
- Daily sales aggregations
- Customer counts and basket sizes
- Weekend vs weekday comparison

**vw_customer_lifetime_value**
- Customer purchase history
- Lifetime value calculation
- Recency analysis

### Sample Insights

```sql
-- 1. Top 10 products by revenue
SELECT * FROM vw_product_sales_summary 
ORDER BY total_revenue DESC LIMIT 10;

-- 2. Monthly sales trend
SELECT 
    TO_CHAR(date_key, 'YYYY-MM') as month,
    SUM(total_revenue) as revenue
FROM vw_daily_sales_summary
GROUP BY month
ORDER BY month;

-- 3. Customer segments
SELECT 
    CASE 
        WHEN total_transactions = 1 THEN 'One-time'
        WHEN total_transactions <= 5 THEN 'Occasional'
        ELSE 'Frequent'
    END as segment,
    COUNT(*) as customers,
    SUM(lifetime_value) as total_value
FROM vw_customer_lifetime_value
GROUP BY segment;
```

---

## ğŸ¨ Architecture Highlights

### Scalability Considerations
- **Modular design** - Easy to add new data sources
- **Batch processing** - Configurable batch sizes
- **Connection pooling** - Efficient database connections
- **Error recovery** - Retry logic with exponential backoff

### Data Engineering Best Practices
- âœ… **Separation of concerns** - Ingestion, transformation, loading
- âœ… **Configuration management** - Environment-based config
- âœ… **Logging and monitoring** - Comprehensive logging
- âœ… **Data validation** - Pydantic models for type safety
- âœ… **Idempotency** - ON CONFLICT handling
- âœ… **Documentation** - Code comments and docstrings

---

## ğŸš€ Future Enhancements

### Phase 2: Cloud Migration
- [ ] Deploy to AWS/GCP/Azure
- [ ] Use cloud-native services (RDS, Cloud SQL)
- [ ] Implement S3/GCS for data lake
- [ ] Add CloudWatch/Stackdriver monitoring

### Phase 3: Advanced Features
- [ ] **Workflow Orchestration** - Apache Airflow integration
- [ ] **Data Quality Framework** - Great Expectations
- [ ] **Incremental Loading** - CDC (Change Data Capture)
- [ ] **Data Lineage** - Track data transformations
- [ ] **Real-time Processing** - Kafka/Pub-Sub integration

### Phase 4: Analytics & BI
- [ ] **Business Intelligence** - Connect Tableau/Power BI/Looker
- [ ] **Machine Learning** - Predictive analytics pipeline
- [ ] **API Layer** - REST API for data access
- [ ] **Automated Reporting** - Scheduled email reports

### Production Readiness Checklist
- [ ] Add unit tests (pytest)
- [ ] Add integration tests
- [ ] Implement CI/CD pipeline (GitHub Actions)
- [ ] Add monitoring and alerting
- [ ] Implement backup and recovery procedures
- [ ] Add performance optimization (indexes, partitioning)
- [ ] Security hardening (secrets management, encryption)
- [ ] Load testing and benchmarking

---

## ğŸ“ Development Guide

### Adding a New Data Source

1. Create ingestion module in `ingestion/`
2. Define Pydantic model in `models.py`
3. Add transformation logic in `transformations/`
4. Update database schema if needed
5. Add to pipeline orchestration

### Adding New Analytics

1. Write SQL query in `sql/analytics_queries.sql`
2. Create view if needed in `schema.sql`
3. Document in README
4. Test with sample data

---

## ğŸ¤ Contributing

This is a personal portfolio project. Feedback and suggestions are welcome!

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

## ğŸ‘¤ Author

**Your Name**
- Portfolio: [Your Portfolio URL]
- LinkedIn: [Your LinkedIn]
- GitHub: [@yourusername](https://github.com/yourusername)

---

## ğŸ™ Acknowledgments

- **FakeStore API** - Sample product data
- **PostgreSQL** - Powerful open-source database
- **Docker** - Containerization platform
- **Python Community** - Amazing libraries and tools

---

## ğŸ“ Contact

For questions or feedback:
- Email: karadimosmixalis@gmail.com
- LinkedIn: [Your LinkedIn Profile]

---

**Built with â¤ï¸ for demonstrating modern data engineering practices**
