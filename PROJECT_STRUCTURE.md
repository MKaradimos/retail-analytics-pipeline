# ğŸ“‚ Project Structure - Visual Guide

## Complete Project Tree

```
retail-analytics-pipeline/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                          # Main documentation (START HERE!)
â”œâ”€â”€ ğŸ“„ ARCHITECTURE.md                    # Detailed architecture docs
â”œâ”€â”€ ğŸ“„ PROJECT_CHECKLIST.md               # Readiness checklist
â”œâ”€â”€ ğŸ“„ Î Î‘Î¡Î‘Î”ÎŸÎ£Î—.md                        # Greek delivery guide
â”œâ”€â”€ ğŸ“„ LICENSE                            # MIT License
â”‚
â”œâ”€â”€ âš™ï¸ Configuration Files
â”‚   â”œâ”€â”€ .env.example                      # Environment template
â”‚   â”œâ”€â”€ requirements.txt                  # Python dependencies
â”‚   â”œâ”€â”€ config.py                         # Configuration management
â”‚   â”œâ”€â”€ Makefile                          # Common commands
â”‚   â””â”€â”€ setup.sh                          # Quick start script
â”‚
â”œâ”€â”€ ğŸ³ Docker Files
â”‚   â”œâ”€â”€ Dockerfile                        # Python app container
â”‚   â”œâ”€â”€ docker-compose.yml                # Multi-service orchestration
â”‚   â””â”€â”€ .dockerignore                     # Docker ignore rules
â”‚
â”œâ”€â”€ ğŸ Core Python Modules
â”‚   â”œâ”€â”€ pipeline.py                       # ğŸ¯ MAIN ORCHESTRATOR
â”‚   â”œâ”€â”€ database.py                       # Database utilities
â”‚   â””â”€â”€ models.py                         # Pydantic validation models
â”‚
â”œâ”€â”€ ğŸ“ ingestion/                         # DATA INGESTION LAYER
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ api_ingestion.py                  # API client (REST)
â”‚   â””â”€â”€ csv_ingestion.py                  # CSV loader (Pandas)
â”‚
â”œâ”€â”€ ğŸ“ transformations/                   # TRANSFORMATION LAYER
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ transform.py                      # Business logic & transformations
â”‚
â”œâ”€â”€ ğŸ“ sql/                               # DATABASE LAYER
â”‚   â”œâ”€â”€ schema.sql                        # DDL: Tables, indexes, views (280 lines)
â”‚   â””â”€â”€ analytics_queries.sql             # 14 pre-built queries (200+ lines)
â”‚
â”œâ”€â”€ ğŸ“ data/                              # DATA FILES
â”‚   â”œâ”€â”€ sales_transactions.csv            # Sample data (500 transactions)
â”‚   â””â”€â”€ generate_sample_data.py           # Data generator script
â”‚
â”œâ”€â”€ ğŸ“ docs/                              # DOCUMENTATION
â”‚   â””â”€â”€ interview_preparation.docx        # Interview prep guide
â”‚
â”œâ”€â”€ ğŸ“ logs/                              # LOGS (auto-generated)
â”‚   â””â”€â”€ pipeline_YYYYMMDD_HHMMSS.log      # Execution logs
â”‚
â””â”€â”€ .gitignore                            # Git ignore rules
```

---

## File Purposes - Quick Reference

### ğŸ¯ Critical Files (Must Know)

| File | Purpose | For Interview |
|------|---------|---------------|
| `README.md` | Complete project documentation | Show first - explains everything |
| `pipeline.py` | Main ETL orchestrator | Core logic - walk through this |
| `models.py` | Data validation models | Show data quality approach |
| `sql/schema.sql` | Data warehouse DDL | Explain star schema here |

### ğŸ“Š Data Flow Through Files

```
1. Data Sources
   â†“
2. ingestion/api_ingestion.py     â† Fetch from API
   ingestion/csv_ingestion.py     â† Load from CSV
   â†“
3. models.py                      â† Validate with Pydantic
   â†“
4. transformations/transform.py   â† Apply business logic
   â†“
5. database.py                    â† Load to warehouse
   â†“
6. sql/schema.sql                 â† Store in tables
   â†“
7. sql/analytics_queries.sql      â† Analyze data
```

---

## Module Dependencies

```
pipeline.py
    â”œâ”€â”€ imports config.py
    â”œâ”€â”€ imports database.py
    â”œâ”€â”€ imports ingestion/
    â”‚   â”œâ”€â”€ api_ingestion.py
    â”‚   â”‚   â””â”€â”€ imports models.py
    â”‚   â””â”€â”€ csv_ingestion.py
    â”‚       â””â”€â”€ imports models.py
    â””â”€â”€ imports transformations/
        â””â”€â”€ transform.py
```

---

## Execution Flow (What Happens When You Run)

```
$ make run
    â”‚
    â”œâ”€â”€ 1. Reads config.py                    (DB credentials, API URLs)
    â”‚
    â”œâ”€â”€ 2. Executes pipeline.py
    â”‚   â”‚
    â”‚   â”œâ”€â”€ Step 1: Initialize Database
    â”‚   â”‚   â””â”€â”€ Runs sql/schema.sql           (Creates tables, views)
    â”‚   â”‚
    â”‚   â”œâ”€â”€ Step 2: Load Products
    â”‚   â”‚   â”œâ”€â”€ api_ingestion.py              (Fetch from FakeStore API)
    â”‚   â”‚   â”œâ”€â”€ models.py                     (Validate with ProductModel)
    â”‚   â”‚   â”œâ”€â”€ transform.py                  (Transform to dim format)
    â”‚   â”‚   â””â”€â”€ database.py                   (Insert to dim_product)
    â”‚   â”‚
    â”‚   â”œâ”€â”€ Step 3: Load Transactions
    â”‚   â”‚   â”œâ”€â”€ csv_ingestion.py              (Load from CSV)
    â”‚   â”‚   â”œâ”€â”€ models.py                     (Validate with TransactionModel)
    â”‚   â”‚   â”œâ”€â”€ transform.py                  (Extract customers & transform)
    â”‚   â”‚   â””â”€â”€ database.py                   (Insert to dim_customer, fact_sales)
    â”‚   â”‚
    â”‚   â”œâ”€â”€ Step 4: Validate Data Quality
    â”‚   â”‚   â””â”€â”€ database.py                   (Run quality checks)
    â”‚   â”‚
    â”‚   â””â”€â”€ Step 5: Generate Summary
    â”‚       â””â”€â”€ Log statistics and results
    â”‚
    â””â”€â”€ 3. Writes to logs/pipeline_*.log      (All execution details)
```

---

## Docker Compose Services

```
docker-compose.yml orchestrates:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Service: postgres                                      â”‚
â”‚  â”œâ”€â”€ Image: postgres:16-alpine                          â”‚
â”‚  â”œâ”€â”€ Purpose: Data warehouse                            â”‚
â”‚  â””â”€â”€ Port: 5432                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Service: pipeline                                      â”‚
â”‚  â”œâ”€â”€ Build: Dockerfile                                  â”‚
â”‚  â”œâ”€â”€ Purpose: ETL application                           â”‚
â”‚  â””â”€â”€ Runs: pipeline.py                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Service: pgadmin                                       â”‚
â”‚  â”œâ”€â”€ Image: dpage/pgadmin4                             â”‚
â”‚  â”œâ”€â”€ Purpose: Database management UI                    â”‚
â”‚  â””â”€â”€ Port: 5050 â†’ http://localhost:5050                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Code Statistics by File

| File | Lines | Purpose |
|------|-------|---------|
| `sql/schema.sql` | 280 | Database DDL |
| `sql/analytics_queries.sql` | 200+ | Analytics queries |
| `pipeline.py` | 270 | Main orchestrator |
| `api_ingestion.py` | 130 | API client |
| `csv_ingestion.py` | 120 | CSV loader |
| `transform.py` | 150 | Transformations |
| `database.py` | 80 | DB utilities |
| `models.py` | 90 | Validation models |
| `generate_sample_data.py` | 140 | Data generator |
| **TOTAL** | **~1,800** | **Complete project** |

---

## File Groups by Concern

### ğŸ“¥ Ingestion Layer
- `ingestion/api_ingestion.py`
- `ingestion/csv_ingestion.py`

### âœ… Validation Layer
- `models.py`

### ğŸ”„ Transformation Layer
- `transformations/transform.py`

### ğŸ’¾ Storage Layer
- `database.py`
- `sql/schema.sql`

### ğŸ“Š Analytics Layer
- `sql/analytics_queries.sql`

### ğŸ­ Orchestration Layer
- `pipeline.py`

### âš™ï¸ Configuration Layer
- `config.py`
- `.env.example`

### ğŸ³ Infrastructure Layer
- `Dockerfile`
- `docker-compose.yml`

### ğŸ“š Documentation Layer
- `README.md`
- `ARCHITECTURE.md`
- `docs/interview_preparation.docx`

---

## Which Files to Show in Interview?

### Must Show (Top 3)
1. **README.md** - "This explains the entire project"
2. **pipeline.py** - "This is the main orchestrator"
3. **sql/schema.sql** - "This is the data model"

### Should Show (if asked)
4. **models.py** - For data validation questions
5. **api_ingestion.py** - For API integration questions
6. **analytics_queries.sql** - For analytics questions

### Can Reference (if needed)
7. **docker-compose.yml** - For containerization questions
8. **ARCHITECTURE.md** - For deep technical discussions

---

## Quick Commands Reference

```bash
# See all files
ls -la

# See Python files only
find . -name "*.py"

# See SQL files only
find . -name "*.sql"

# Count lines of code
find . -name "*.py" -o -name "*.sql" | xargs wc -l

# View project tree
make help  # Shows all available commands
```

---

## Development Workflow

```
1. Clone/Download Project
   â†“
2. Run setup.sh (or make quickstart)
   â†“
3. Edit .env if needed
   â†“
4. Run make run
   â†“
5. Check logs/
   â†“
6. Query database (make psql)
   â†“
7. View results in pgAdmin
```

---

## Testing Workflow

```
1. Generate fresh data
   python data/generate_sample_data.py
   â†“
2. Clean database
   make down && make up
   â†“
3. Run pipeline
   make run
   â†“
4. Verify results
   make psql
   SELECT COUNT(*) FROM fact_sales;
```

---

**Remember**: This structure follows **separation of concerns** - each file/folder has ONE clear purpose. Perfect for explaining in interviews! ğŸ¯
